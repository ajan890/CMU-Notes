% document formatting
\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}
\usepackage[T1]{fontenc}
\usepackage{xcolor}

% math symbols, etc.
\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{dsfont}

% lists
\usepackage{enumerate, enumitem}
\usepackage{tabularx, multirow}

% images
\usepackage{graphicx} % for images
\usepackage{tikz}

% code blocks
% \usepackage{minted, listings} 

% verbatim greek
\usepackage{alphabeta}

\newcommand{\dd}{\text{d}}
\newcommand{\var}{\text{Var}}


\graphicspath{{./assets/images/Module 18}}

\title{02-680 Module 18 \\ \large{Essentials of Mathematics and Statistics}}
 
\author{Aidan Jan}

\date{\today}

\begin{document}
\maketitle

\section*{Parameter Estimation and Maximum Likelihood Estimation}
Consider a dataset $X_1, X_2, \dots, X_n$ of independent and identically distributed random varialbes from the same unknown distribution.  That is, for each $X_{i}$, the underlying distributions have the same $\mu$ and $\sigma$.\\\\
Let $\bar{X_n}$ be the average of the actual value of the observations:
\[\bar{X_n} = \frac{X_1 + X_2 + \cdots + X_n}{n} = \frac{1}{n} \sum_{i = 1}^n X_i\]
Note this is different from the expected value of the underlying distribution $\mu$.  Note also that $\bar{X_n}$ is also a random variable in an of itself.

\subsection*{Statistical Inference}
We will use statistical inference or \textbf{learning} to try to make the models match the observations we've made about the world.

\subsubsection*{Example: Coin Flip Simulator}
Consider a coin flip simulator which usually outputs ``heads'' or ``tails'', but once in every 1000 flips, it outputs a nonsense number.  The simulator acts as a data generator, producing observations we can see and measure.

\subsection*{Model vs. Reality}
On the other side, we have a set of models - mathematical representations of how we think the data might have been generated.  Each model has its own set of parameters, e.g., the probability of heads in a biased coin.

\subsection*{What is Statistical Inference?}
\textbf{Statistical Inference} is the task of:
\begin{itemize}
	\item using the data (observations)
	\item to estimate or ``learn'' the parameters of the model
	\item so that the model better matches the observed data
\end{itemize}
It's about bridging the gap between theory (models) and reality (data).\\\\
Assume you have a set $D = X_1, X_2, \dots, X_n$ of independent and identically distributed variables for some unknown distribution.  Let $F$ be a statistical model defined as a set of probability distributions.  We will focus on parametric models defined as 
\[F\{f(x | \theta) \::\: \theta \in \Theta\}\]
where $\theta = \{\theta_1, \theta_2, \dots, \theta_k\}$ are unknown parameters $\theta \in \Theta$.\\\\
The task is to find a distribution $\hat{f} \in F$ that models the phenomenon well, which includes modeling the associated $\theta$.\\\\
We want to do this in a way that has:
\begin{itemize}
	\item the ability to generalize well
	\item the ability to incorporate prior knowledge and assumptions
	\item the ability to scale.
\end{itemize}

\subsection*{Data Likelihood}
Bayes theorem (written slightly differently) states:
\[p(hypothesis | data) = \frac{p(data | hypothesis) p(hypothesis)}{p(data)}\]
If we can figure out all the values on the right, we can get the probaiblity of interest on the left (how good our model parameters are given the data).\\\\
Let's start by looking at the most complicated component of the right side: $p(data | hypothesis)$, or in our case $p(D | \theta)$.  Remember that the elements of $D$ are identically and independently distributed, thus we can rewrite it:
\[p(\mathcal{D}|\theta) = p(X_1, X_2, \dots, X_n | \theta) = \prod_{i = 1}^n p(X_i | \theta)\]

\subsection*{Log Likelihood}
Taking products of probabilities of the values can get very small.  Therefore, we often take the log likelihood of the function.  This turns the product into a sum.
\[\log(p(X_1, X_2, \dots, X_n | \theta)) = \sum_{i = 1}^n \log(p(X_i | \theta))\]

\section*{Maximum Likelihood Estimation (MLE)}
The maximum likelihood estimate (MLE) is a way to estimate the value of a parameter of interest $\theta$.  This is the most frequently used method for parameter estimation.\\\\
Thinking about how we \textbf{maximize a function} (or really find the parameter that maximizes the function), we need to take its derivative and check the 0 points.
\[\frac{\dd}{\dd \theta} p(\mathcal{D} | \theta) = 0\]
(but we also need to ensure its a max and not a min or saddle.)
If there is no critical point, then we will use the maximum allowable parameter value.





\end{document}