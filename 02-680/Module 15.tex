% document formatting
\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}
\usepackage[T1]{fontenc}
\usepackage{xcolor}

% math symbols, etc.
\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{dsfont}

% lists
\usepackage{enumerate, enumitem}
\usepackage{tabularx, multirow}

% images
\usepackage{graphicx} % for images
\usepackage{tikz}

% code blocks
% \usepackage{minted, listings} 

% verbatim greek
\usepackage{alphabeta}

\newcommand{\dd}{\text{d}}

\graphicspath{{./assets/images/Module 15}}

\title{02-680 Module 15 \\ \large{Essentials of Mathematics and Statistics}}
 
\author{Aidan Jan}

\date{\today}

\begin{document}
\maketitle

\section*{Multiple Random Variables}
Sometimes, we need to consider interactions between more than one random variable.  These situations arise in many real-world and statistical applications.  Consider tossing a (fair) coin 3 times, and define two random variables:
\begin{itemize}
	\item $X$ - the number of heads in the first toss
	\item $Y$ - the number of heads in all 3 tosses
\end{itemize}
We want to know the \textbf{joint probability} (that is, the probability $p(X = x, Y = y)$)
\begin{center}
\bgroup
\def\arraystretch{1.5}
\begin{tabular}{cc|cccc}
\textbf{} & \textbf{} & \multicolumn{4}{c}{$Y$} \\
 &  & 0 & 1 & 2 & 3 \\ \hline
\multirow{2}{*}{$X$} & 0 & $\frac{1}{8}$ & $\frac{1}{4}$ & $\frac{1}{8}$ & $0$ \\
 & \multicolumn{1}{l|}{1} & \multicolumn{1}{l}{$0$} & $\frac{1}{8}$ & $\frac{1}{4}$ & $\frac{1}{8}$
\end{tabular}
\egroup
\end{center}

\subsection*{Marginal Probabilities}
What if we're given a \textbf{joint distribution} and want to compute the probabilities of individual variables?  This leads us to \textbf{marginal probabilities}:\\\\
For example, unfair coins.  Let
\begin{itemize}
	\item $X$ = outcome of coin 1
	\item $Y$ = outcome of coin 2
\end{itemize}
These are two unfair coins modeled as \textbf{random variables.}  Suppose we're given the following joint probabilities:
\begin{center}\bgroup
\def\arraystretch{1.5}
\begin{tabular}{cc||c|c|c}
\textbf{} & \textbf{} & \multicolumn{2}{c|}{$Y$} &  \\
 &  & \texttt{Heads} & \texttt{Tails} &  \\ \hline \hline
\multirow{2}{*}{$X$} & \texttt{Heads} & $\frac{1}{10}$ & $\frac{2}{10}$ & $\frac{3}{10}$ \\ \cline{2-5}
 & \texttt{Tails} & $\frac{3}{10}$ & $\frac{4}{10}$ & $\frac{7}{10}$ \\ \hline
 &  & $\frac{4}{10}$ & $\frac{6}{10}$ & 
\end{tabular}
\egroup
\end{center}
If we want to determine, say $P(X = \text{Tails})$, it turns out we can sum over all the possibilities of $Y$:
\[P(X = \texttt{Tails}) = \sum_{y \in \{\texttt{Heads, Tails}\}} p(X = \texttt{Tails}, Y = y) = \frac{3}{10} + \frac{4}{10} = \frac{7}{10}\]
Doing all the math, both coins are biased toward Tails, the first coin $(X)$ more-so.
\subsubsection*{What are Marginal Probabilities?}
They are individual probabilities of one variable, found by summing over the other:
\begin{align*}
P(X = \texttt{Heads}) &= \frac{1}{10} + \frac{2}{10} = \frac{3}{10} \\
P(X = \texttt{Tails}) &= \frac{3}{10} + \frac{4}{10} = \frac{7}{10} \\
P(Y = \texttt{Heads}) &= \frac{1}{10} + \frac{3}{10} = \frac{4}{10} \\
P(Y = \texttt{Tails}) &= \frac{2}{10} + \frac{4}{10} = \frac{6}{10}
\end{align*}

\section*{Continuous Random Variables}



\end{document}