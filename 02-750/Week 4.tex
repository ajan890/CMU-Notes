% document formatting
\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}
\usepackage[T1]{fontenc}
\usepackage{xcolor}

% math symbols, etc.
\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{dsfont}

% lists
\usepackage{enumerate}

% images
\usepackage{graphicx} % for images
\usepackage{tikz}
\usetikzlibrary{fit}

% code blocks
\usepackage{minted, listings} 

% verbatim greek
\usepackage{alphabeta}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\graphicspath{{./assets/images/}}

\title{02-750 Week 4 \\ \large{Automation of Scientific Research}}
 
\author{Aidan Jan}

\date{\today}

\begin{document}
\maketitle

\subsection*{Quantifying Committee Disagreement (continuued)}
Let $\mathcal{M} = \{h_1, \cdots, h_p\}$ be a diverse set of models (i.e., committee members)
\subsubsection*{Soft Vote Entropy}
\begin{itemize}
	\item Method 2: \textbf{Soft Vote Entropy} (for classification)
	\[x^*_{SVE} = \argmax_{x \in \mathcal{U}} - \sum_{y \in \mathfrak{C}} P_\mathcal{M} (y | x) \log P_\mathcal{M} (y | x)\]
    where $P_\mathcal{M} (y | x) = \frac{1}{|\mathcal{M}|} \sum_{h \in \mathcal{M}} P_h (y | x)$ is the consensus probability that the true label is $y \in \mathfrak{C}$ and $P_h(y | x)$ is the probability that model $h$ assigns label $y$ for point $x \in \mathcal{U}$
    \item Soft vote entropy takes the confidence of each committee member into consideration.    
\end{itemize}
\subsubsection*{KL Divergence}
\begin{itemize}
    \item Method 3: \textbf{Kullback-Leibler (KL) Divergence}
    \[x^*_{KL} = \argmax_{x \in \mathcal{U}} \frac{1}{|\mathcal{M}|} \sum_{h \in \mathcal{M}} D(P_h (y | x) || P_{\mathcal{M}} (y | x))\]
    where $D(P_h (y | x) || P_\mathcal{M}(y | x)) = \sum_{y' \in \mathfrak{C}} P_h (y' | x) \log \frac{P_h (y' | x)}{P_\mathcal{M} (y' | x)}$ and $P_\mathcal{M} (y | x) = \frac{1}{|\mathcal{M}|} \sum_{h \in \mathcal{M}} P_h (y | x)$ is the ``consensus'' probability that the true label is $y \in \mathfrak{C}$ (as defined in the Soft Vote Entropy approach) for point $x \in \mathcal{U}$.
\end{itemize}
\textbf{KL Divergence} measures the \textbf{average divergence} of each committee member's prediction(s) from the consensus distribution.

\subsubsection*{Comparing Soft Vote Entropy vs. KL Divergence}
\begin{center} 
	\includegraphics*[width=0.8\textwidth]{W4_1.png} 
\end{center}
\begin{itemize}
	\item Blue pie charts represent label probabilities for each of three committee members.  Red pie charts are the consensus probabilities.
	\item Soft vote entropy cannot distinguish between these two scenarios, but the KL Divergence method can.
\end{itemize}
Example using logistic regression:
\begin{center} 
	\includegraphics*[width=0.8\textwidth]{W4_2.png} 
\end{center}
For regression, disagreement is typically measured as the variance among the predictions of the committee members
\begin{center} 
	\includegraphics*[width=0.8\textwidth]{W4_3.png} 
\end{center}

\subsection*{Expected Model Change}
Given $D_L = \{(x_1, y_1), \cdots, (x_n, y_n)\}$, many learning algorithms optimize parameters by iteratively following the gradient of an objective function: $\theta_{t + 1} = \theta_t - \mu \nabla \mathcal{L}_\theta (D_L)$, where $\mu$ is the learning rate and $\nabla \mathcal{L}_\theta (D_L)$ is the gradient of the loss function.  For example, mean squared error for linear regression:
\begin{itemize}
	\item Model: $y = b_0 + b_1 x; \theta = (b_0, b_1)$
	\item Mean squared error: $\mathcal{L}_{\theta = (b_0, b_1)} (D_L) = \frac{1}{n} \sum_{i = 1}^n [y_i - (b_0 + b_1 x_i)]^2$
	\item Gradient: $\nabla \mathcal{L}_\theta (D_L) = \left(\frac{\partial \mathcal{L}}{\partial b_0}, \frac{\partial \mathcal{L}}{\partial b_1}\right)$
	\begin{align*}
        \frac{\partial \mathcal{L}}{\partial b_0} &= -\frac{2}{n} \sum_{i = 1}^n (y_i - (b_0 + b_1 x_i)) \\
        \frac{\partial \mathcal{L}}{\partial b_1} &= -\frac{2}{n} \sum_{i = 1}^n x_i(y_i - (b_0 + b_1 x_i)) \\
    \end{align*}
    \item The \textbf{magnitude of the gradient} is proportional to the change in the model parameters during each iterative step, thus, we can \textit{also} use it to select points.
    \item This approach selects instances that are more likely to significantly change the parameters.
\end{itemize}
Let $\nabla \mathcal{L}_\theta (D_\mathcal{L} \cup \{(x, \hat{y})\})$ be the gradient of the loss function if we were to add the ``labeled'' point $(x, \hat{y})$ to $D_\mathcal{L}$, where $\hat{y}$ is a predicted label.
\begin{itemize}
	\item Since we don't know $x$'s true label, we compute the \textbf{expected magnitude}.
	\[x^*_{EGL} = \argmax_{x \in \mathcal{U}} - \sum_{y \in \mathfrak{C}} P_\theta (y | x) \Vert \nabla \mathcal{L}_\theta (D_\mathcal{L} \cup \{(x, y)\}) \Vert\]
    where $\Vert \cdot \Vert$ is some norm function.
    \begin{itemize}
	    \item $P_\theta (y | x)$ is a class label prediction for the current $x \in \mathcal{U}$
	    \item $\nabla \mathcal{L}_\theta (D_\mathcal{L} \cup \{(x, y)\})$ is the new gradient that would be obtained by adding the current instance, $(x, y)$ to $D_\mathcal{L}$
    \end{itemize}
\end{itemize}

\subsubsection*{Disadvantages}
\begin{itemize}
	\item Expected model change can be expensive, computationally, because the algorithm needs to compute the gradient for each (instance, label) pair
	\begin{itemize}
	    \item If the objective function is differentiable, the cost of computing the gradient is proportional to the dimensionality of the data.
	    \item If the objective function is not differentiable, you can use a derivative-free approach for estimating and following the gradient (ex. Nelder-Mead) but those tend to be expensive.
    \end{itemize}
    \item That said, in the context of scientific research, the cost of performing these calculations may be much smaller than performing the experiments themselves.
\end{itemize}

\subsection*{Minimizing Expected Risk (in Machine Learning)}
In machine learning, a \textbf{risk objective} (aka \textbf{decision rule}) defines what 'best' means.  For example, expected loss, minimax loss, maximum likelihood, etc.
\begin{itemize}
	\item Risk refers to the idea of expected generalization error
	\begin{itemize}
	    \item i.e., the model's error on data it \textbf{was not} trained on
    \end{itemize}
	\item Notice that none of the previous query selection strategies consider risk
	\begin{itemize}
	    \item Uncertainty sampling only considers the confidence of the current model
	    \item QBC only considers the degree of disagreement amongst the committee members
	    \item Expected model change only considers the size of change in parameters
    \end{itemize}
	\item This is odd because generalization error is the only thing that matters
	\begin{itemize}
	    \item The next method attempts to selected points that are predicted to reduce risk.
	    \item It does this by explicitly learning separate models, \textbf{each conditioned on one of the possible labels for each unlabeled instance}
    \end{itemize}   
\end{itemize}
For example, consider the \textbf{Expected 0-1 loss} (i.e., classification error).  Here, we are assuming an unlabeled pool $\mathcal{U}$
\[x^*_{ER} = \argmin_{x \in \mathcal{U}} \sum_{y \in \mathfrak{C}} P_\theta (y | x) \left(\sum_{x' \in \mathcal{U}} 1 - P_{\theta + (x, y)} (\hat{y} | x')\right)\]
\begin{itemize}
	\item $P_\theta (y | x)$ is the class label prediction for the current $x \in \mathcal{U}$.
	\item $\hat{y}$ is the most likely label for $x'$ under a new model trained on $D_\mathcal{L} \cup \{(x, y)\}$
	\item $P_{\theta + (x, y)} (\hat{y} | x')$ is the conditional probability of under the new label
	\item $1 - P_{\theta + (x, y)} (\hat{y} | x')$ is the expected 0-1 error for current instance $(x', \hat{y})_{x' \neq x}$
	\item The term in the parenthesis in the expected 0-1 loss over $\mathcal{U}$ for a new model trained on $D_\mathcal{L} \cup \{(x, y)\}$
\end{itemize}
Note: like expected model change, we do not know the true label for each query instance ($x \in \mathcal{U}$), so we approximate using expectation over all possible labels $(y \in \mathfrak{C})$ under the current model $\theta$.  The objective here is to reduce the expected total number of incorrect predictions.\\\\
Minimizing the expected risk is \textbf{much more expensive} than any of the other query selection methods we have seen, because you need to train many models on each iteration
\begin{itemize}
	\item A binary logistic regression model would require O($UNG$) time-complexity simply to choose the next query.
	\begin{itemize}
	    \item $U$ - size of unlabeled pool (i.e., $|\mathcal{U}|$)
	    \item $N$ - size of the current training set (i.e., $|D_\mathcal{L}$)
	    \item $G$ - number of gradient computations required by the optimization procedure until convergence
    \end{itemize}
    \item Once again, in the context of scientific research, the cost of performing these calculations may be much smaller than performing the experiments themselves.
\end{itemize}

\subsection*{Density-based Sampling}
\begin{itemize}
	\item The query selection strategies we have studied so far are \textbf{myopic}, in the sense that they select instances without considering the relationships between the points in the domain/pool
    \item This could potentially lead to problems.  For example, suppose a far outlier point that happens to be on the decision boundary between two labels.  Is it really worth getting the label for that point if it is not representative of any other points (e.g., an outlier)?
\end{itemize}
For example, consider the information density framework, which is a general weighting technique.  Here, we are assuming an unlabeled pool $\mathcal{U}$.
\[x^*_{ID}= \argmax_{x \in \mathcal{U}} \phi_A (x) \left(\frac{1}{|\mathcal{U}|} \sum_{x' \in \mathcal{U}} sim(x, x')\right)^\beta\]
\begin{itemize}
	\item Here, $\phi_A(x)$ is a function that returns the utility of the point $x \in \mathcal{U}$.
	\item $\phi$ can be any query strategy that we have covered (e.g., uncertainty sampling, query by committee)
	\item Function $sim(x, x')$ returns some measure of similarity between point $x$ and $x'$.
	\begin{itemize}
	    \item The term in the parenthesis in the \textbf{average similarity} of $x$ to all other instances in $\mathcal{U}$, which is a measure of the local density around $x$.
	    \item $\beta$ is a parameter that controls the relative importance of the density term.
    \end{itemize}
    \item A variant of this approah might first cluster $\mathcal{U}$ and compute average similarity to instances in the same cluster.
\end{itemize}

\subsubsection*{Exploiting Structure in the Data}
\begin{itemize}
	\item Density based sampling is an example of a query selection method that exploits the \textbf{structure} in an (unlabeled) data pool
	\item Later in the semester, we will study active learning algorithms that cluster the unlabeled samples, and then use the clusters to guide query selection.
\end{itemize}
\begin{center} 
	\includegraphics*[width=0.8\textwidth]{W4_4.png} 
\end{center}

\subsection*{Summary: Heuristic Query Selection Strategies}
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Method} & \textbf{Advantages} & \textbf{Disadvantages} \\ \hline
Uncertainty Sampling & Fast and easy to implement & \begin{tabular}[c]{@{}c@{}}Myopic and may become\\ overconfident\end{tabular} \\ \hline
Query by Committee & East to implement & \begin{tabular}[c]{@{}c@{}}Myopic and requires multiple\\ models\end{tabular} \\ \hline
Expected Change & \begin{tabular}[c]{@{}c@{}}May accelerate convergence to best \\ model\end{tabular} & Multiple gradient calculations \\ \hline
Risk Minimization & \begin{tabular}[c]{@{}c@{}}Optimizes the objective we actually\\ care about\end{tabular} & Can be very expensive \\ \hline
Density-based & Not myopic & \begin{tabular}[c]{@{}c@{}}Requires $\Omega(n^2)$ work to compute\\ ${n \choose 2}$ pairwise similarities\end{tabular} \\ \hline
\end{tabular}
\end{center}
There are many different query selection methods
\begin{itemize}
	\item The ones we studied this week are merely heuristics
	\item We can't prove anything about them, with respect to statistical consistency or efficiency
\end{itemize}
Later, we will see several query selection methods that come with formal guarantees

\section*{Batch Mode Active Learning}
\begin{itemize}
	\item \textbf{Intuition:} Select a set of $k$ unlabeled instances that maximize (or minimize) some criterion (e.g., Fisher information)
	\begin{itemize}
	    \item A simple strategy: Select the top $k$ most informative instances
	    \item Some of the selected instances could be similar to each other, and therefore do not provide additional information for model updating
    \end{itemize}
    \item \textbf{Goal:} Select instances that are informative such that each selected instances is different from the others and provides unique information
    \item \textbf{Challenge:} How to efficiently identify the subset of unlabeled instances under specified criterion?
    \begin{itemize}
	    \item Computationally complex to consider all combinations of potential instances
	    \item Do not know labels
    \end{itemize}
\end{itemize}

\subsection*{Optimal Design of Experiments}
\begin{itemize}
	\item Batch - choose multiple instances in one group for experimentation (One or two batches for DOE.)
\end{itemize}

\subsection*{Is Active Learning an Appropriate Strategy?}
Experiment execution time takes much, much longer than modeling and instance selection time.
\begin{itemize}
	\item We have already improved efficiency of running experiments
	\item Miniaturization leads to more experimentation.  (e.g., Use a 1536 well plate instead of a 384 well plate)
	\item Combining miniaturization with instance selection allows for lower per compound test time even if the total time spent is still lengthy!
	\item Need to choose compounds in batches to maximize efficiency!
\end{itemize}

\subsection*{High-Level Query Selection Criteria: Exploration vs. Exploitation}
\textbf{Exploration}
\begin{itemize}
	\item Select designs that are from unexplored regions of the design space, $\mathcal{X}$
	\begin{itemize}
	    \item \textbf{Strength:} We gain knowledge that might help us build a better model (i.e., one that is better at directing us towards optimal designs)
	    \item \textbf{Weakness:} We may waste effort (opportunity loss)
    \end{itemize}
\end{itemize}
\textbf{Exploitation}
\begin{itemize}
	\item Select designs that are predicted to be optimal under the current model, $h$
	\begin{itemize}
	    \item \textbf{Strength:} Leverage the information we have
	    \item \textbf{Weakness:} We may get stuck in a local optimum (opportunity loss)
    \end{itemize}
\end{itemize}
\begin{center} 
	\includegraphics*[width=0.8\textwidth]{W4_5.png} 
\end{center}
\begin{itemize}
	\item What happens if our next 3 points are the ones on the line (e.g., all exploration)?
	\begin{itemize}
	    \item Not a great idea, since we only need one of those points to resolve the other two!  We will waste two of the three experiments in that case.
    \end{itemize}
	\item What happens if our next 3 points are all as far away as possible (e.g., all exploitation)?
	\begin{itemize}
	    \item If you do this, then you have absolutely no clue where the decision boundaries are!
    \end{itemize}
\end{itemize}
Ideally, we want to pick some points for exploration and some points for exploitation.

\subsection*{Diversity-Based Sampling Strategies}
\begin{itemize}
	\item Cluster-based sampling - cluster the data and choose representative instances from each cluster
	\item How could you choose $n$ instances in a \textbf{hierarchical clustering} mode?
	\begin{itemize}
	    \item Algorithm: Identify level of tree yielding $n$ clusters.  Choose randomly from those clusters.
    \end{itemize}
    \item How could you choose $n$ instances in a \textbf{k-means} mode?
    \begin{itemize}
	    \item Algorithm: Set $k = n$, and choose one instance from each cluster.
    \end{itemize}
\end{itemize}

\subsection*{Discriminative Batch Mode Active Learning}
Key features
\begin{itemize}
	\item It presents a framework for ``batch mode active learning'' that applies the Fisher information matrix to measure the overall informativeness for a set of unlabeled instances
	\item Propose an efficient greedy algorithm that is based on the property of \textbf{submodular functions}
	\item Empirical studies show that the proposed batch mode active learning algorithm is more effective than the traditional algorithms for active learning.
\end{itemize}

\subsubsection*{Batch Mode Active Learning Set-up}
Approach:
\begin{itemize}
	\item Given that the Fisher information matrix represents the overall uncertainty of a classification model, the goal is to search for a set of instances that can most efficiently reduce the Fisher information matrix
\end{itemize}
Notation:
\begin{itemize}
	\item $p(x)$ be the distribution of all unlabeled instances
	\item $q(x)$ be the distribution of unlabeled instances that are chosen for labeling
	\item $\alpha$ denotes the parameters of the classification model
	\item $I_p(\alpha)$ and $I_q(\alpha)$ denote the Fisher information matrix of the classification model for the distribution $p(x)$ and $q(x)$, respectively.
	\item $D = (x_1, \cdots, x_n)$ be the unlabeled data
	\item $S = (x_1^5, \cdots, x_k^5)$ be the subset of selected instances
\end{itemize}
The Fisher matrices $I_p(\alpha)$ and $I_q(\alpha)$ can be computed as:
\begin{align*}
    I_p(\hat{\alpha}) &= \frac{1}{n} \sum_{x \in \mathcal{D}} \pi(x) (1 - \pi(x)) xx^T + \delta I_d \\
    I_q(S, \hat{\alpha}) &= \frac{1}{k} \sum_{x \in S} \pi(x) (1 - \pi(x)) xx^T + \delta I_d \\
\end{align*}
where
\[\pi(x) = p(- | x) = \frac{1}{1 + \exp(\hat{\alpha}^T x)}\]
\begin{itemize}
	\item $\hat{\alpha}$ stands for the classification model that is estimated from the labeled instances
	\item $I_d$ is the identity matrix of size $d \times d$
	\item $\delta << 1$ is the smoothing parameter
	\item $\delta I_d$ is added to the estimation of $I_p(\hat{\alpha})$ and $I_q(\hat{\alpha})$ to prevent them from being singular matrices
\end{itemize}
The final optimization problem for batch mode active learning is formulated as follows:
\[S^* = \argmin_{S \subseteq D \land |S| = k} \text{ tr}(I_q(S, \hat{\alpha})^{-1} I_p(\alpha))\]
\textbf{Challenge:} The number of candidate sets for $S$ is exponential in the number of unlabeled examples $n$

\subsection*{Batch Mode Active Learning: Algorithm}
The key idea of this approach is to explore a general theorem about submodular functions (Newhauser et al., 1978)
\begin{itemize}
	\item Consider the optimization problem that searches for a subset $S$ with $k$ elements to maximize a set function $f(S)$
\begin{verbatim}
    Initialize S = empty set
    For i = 1, 2, ..., k
        Compute x* = argmax_{x not in S} f(S + x) - f(S)
        Set S += x*
\end{verbatim}
    \item If $f(S)$ is (i) a nondecreasing submodular function, and (ii) $f(\emptyset) = 0$, then the above greedy algorithm will guarantee a performance $\left(1 - \frac{1}{e}\right) f(S^*)$, where $S^* = \argmax_{|S| = k} f(S)$ is the optimal subset
\end{itemize}

\subsection*{Meta-Learning for Batch Mode Active Learning}
\begin{center} 
	\includegraphics*[width=\textwidth]{W4_6.png} 
\end{center}

\subsection*{Summary of Batch Selection Strategies}
\begin{itemize}
	\item Batch mode strategies can improve the efficiency of scalable processes in an active learning context
	\item They mostly require consideration of exploration and exploitation in the design of the batches
\end{itemize}

\end{document}

