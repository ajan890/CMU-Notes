% document formatting
\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}
\usepackage[T1]{fontenc}
\usepackage{xcolor}

% math symbols, etc.
\usepackage{amsmath, amsfonts, amssymb, amsthm}

% lists
\usepackage{enumerate}

% images
\usepackage{graphicx} % for images
\usepackage{tikz}

% code blocks
\usepackage{minted, listings} 

% verbatim greek
\usepackage{alphabeta}

\newcommand{\dd}{\text{d}}

\graphicspath{{./assets/images/Week 8}}

\title{02-712 Week 8 \\ \large{Biological Modeling and Simulation}}
 
\author{Aidan Jan}

\date{\today}

\begin{document}
\maketitle

\section*{Partial Differential Equations (PDEs)}
A lot of times we want to model populations in space.  Normal differential equations cannot account for this since they only handle one variable, while if multiple populations are present in the same area, they may have an influence on each other.\\\\
Let's start with an example:

\subsection*{Example:}
A beneficial mutant appears at one point along a long habitat (e.g., coastline, intestinal tract).  It grows where it is, and moves randomly through the continuous habitat.  We want to know how fast does this mutant take over the space.  What is the mutant progress over the space over time?\\\\
Define a state variable $u(x, i)$, which represents the proportion of the population at positive $x$ that carries the mutant at time point $t$.  Let the rate of change be
\[\frac{\partial u}{\partial t} = D \partial_{xx}u + ru(1 - u)\]
where $D$ is a diffusive term or diffusive coefficient.  $\partial_{xx}$ represents the second derivative of the function in terms of $x$.\\\\

\subsubsection*{SI Model with PDEs}
\begin{align*}
    \partial_t S &= D_S \partial_{xx} S - \beta SI \\
    \partial_t I &= D_I \partial_{xx} I + \beta SI - \gamma I
\end{align*}
$\gamma$ is the death rate, and $\beta$ is the rate at which susceptible organisms become infected.\\\\
If we imagine this SI model on a long habitat like the example above, the mutants can spread over the space as time progresses.  By taking the derivative of their change in $x$, we can describe where the mutants are located.  
\begin{itemize}
    \item Suppose we cut up the $x$ length into $\Delta D$ (small) intervals, and also write the progression of time iteratively (e.g., current time is $t$, and the next time step $t + \Delta t$), as a function on our current state.
    \[u(x, t + \Delta t) = Du(x - \Delta x, t) + u(x, t)(1 - 2D) + D u(x + \Delta x, t)\]
    \item In other words, the number of individuals at position $x$ at time $t + \Delta t$ is equal to the number of individuals that moved from the left (first term), the amount that stayed at the current location (second term), and the amount coming from the right side (last term)
    \item The $(1 - 2D)$ comes from the (current population) minus (the diffusion to the two sides), which therefore represents the number of individuals stayed at the same location.
\end{itemize}
Now, how do we write the $\frac{\partial u}{\partial t}$ from the example above in terms of $D \partial_{xx} u$?  To get there, we take a taylor expansion.
\begin{align*}
    u(x, t + \Delta t) &= D\left[u(x, t) - \frac{\partial u(x, t)}{\partial x} \Delta x + \frac{\partial^2 u(x, t)}{2\partial x^2} (\Delta x)^2 + \theta(\Delta x^3)\right] \\
    &\quad + u(x, t)(1 - 2D) \\
    &\quad + D\left[u(x, t) + \frac{\partial u(x, t)}{\partial x} \Delta x + \frac{\partial^2 u(x, t)}{2\partial x^2} (\Delta x)^2 + \theta(\Delta x^3)\right]\\
    u(x, t + \Delta t) - u(x, t) &= \frac{\partial u(x, t)}{\partial x} \Delta t + \frac{\partial u(x, t)}{\partial x^2}D (\Delta x)^2\\
    \frac{u(x, t + \Delta t) - u(x, t)}{\Delta t} &= \frac{\partial^2 u(x, t)}{\partial x^2} D
\end{align*}
Since $\frac{\partial^2 u(x, t)}{\partial x^2}$ can be written as $\partial_{xx}u$, we obtain the $D\partial_{xx}u$ term from the example at the beginning.

\subsection*{Numerical Derivatives of PDEs}
Similarly to how we take derivatives in one dimension by cutting into tiny intervals, we can take derivatives in two dimensions by cutting it into a fine mesh, in this case cutting the time and $x$ dimensions into tiny intervals.
Let
\begin{align*}
    x_m &= x_0 + m \Delta x \quad\text{ where } m = 0, 1\\
    t_j &= t_0 + j \Delta t \quad\text{ where } j = 0, 1
\end{align*}
Also, let an instance of $u(x, t)$, e.g., population at a point $x$ at a given time $t$ be represented in our mesh as $u(x_m, t_j) \equiv u_{m, j}$.  Now, we can express a given point as a function of neighboring points and previous times.
\begin{itemize}
    \item If we have the current spatial distribution at the current time, we can use the information to calculate the distribution at time $t + \Delta t$, or $u_{m, j + 1}$
\end{itemize}

\section*{Brownian Motion}
Brownian motion describes how a particle moves in a solution subject to random thermal fluctuations.  Brownian motion tends to be very jagged, as the particles can very quickly change direction.

\subsection*{Symmetric Random Walk}
The symmetric random walk is defined by the equation
\[x_m = \sum_{k = 1}^m \alpha_k\]
As we make the step interval smaller, the curve appears less smooth or jagged.  A random walk with and infinitely small step size (e.g., size $\dd x$ would simulate a Brownian motion.)\\\\
Random walks do have other properties:
\begin{itemize}
    \item The expected value of random walk in 1 dimension is zero.  This is because there is an equal chance of moving up and down.
    \item The variance scales proportionally to the number of steps you take ($m$).
    \item It has a fractal-like pattern, where if you zoom in, the line is always jagged, never smooth.
\end{itemize}

\subsection*{Brownian Motion as Noise}
Suppose we have a colony of bacteria that grows exponentially.  
\begin{itemize}
	\item The issue is, bacteria don't necessarily increase by exactly a certain number every time step.  The general population will follow the $c \cdot e^x$ trend, but the exact count will almost never fall exactly on the line.
	\item To model this noise, we can add Brownian motion
\end{itemize}
We can write the original rate of change of the bacteria as
\[\frac{\dd S}{\dd t} = r \cdot S_0^{ert}\]
With the added noise, we now have
\[\frac{\dd S}{\dd t} = S_0 e^{ert} \left(r + \beta \frac{\dd \beta}{\dd t}\right)\]
Where $\beta(t)$ represents the noise function.  Since the growth rate of bacteria is the derivative of its population (and noise is applied to the population), the rate of change of population involves the rate of change of the noise.
\begin{itemize}
	\item The one major issue with this is that the noise function is not differentiable!
	\item Since every step is random, the function forms a cusp at every time step; the slope is not conceptually defined.
\end{itemize}
This acts as the inspiration to stochastic integration.

\subsection*{Stochastic Integrals}

\subsection*{Example: Neutral Drift in an Evolving Population}
Suppose we have a population of bacteria of size $N$ with two mutants, $A$ and $B$.  Let $A$ and $B$ be neutral variants - that is, neither are beneficial or detrimental in the face of natural selection.  At every time step, a random individual is chosen for reproduction.  One random indivudual is chosen for death.  The identical offspring of the first individual replaces the second.
\begin{itemize}
	\item The total population does not change.  However, the counts of $A$ and $B$ may change.
	\item So we can analyze this system, let the population of $A$ be $i$, and the population of $B$ be $N - i$.  $i$ is a stochastic variable; it fluctuates randomly.
	\item First, note which possibilities can happen on each time step:
	\begin{itemize}
	    \item $A$ reproduces, and $A$ dies.  ($i_{t + 1} = i_t$, with probability $(i / N)^2$)  
	    \item $B$ reproduces, and $B$ dies.  ($i_{t + 1} = i_t$, with probability $((N - i) / N)^2$)
	    \item $A$ reproduces, and $B$ dies.  ($i_{t + 1} = i_t + 1$, with probability $(i(N - i) / N^2)$)
	    \item $B$ reproduces, and $A$ dies.  ($i_{t + 1} = i_t - 1$, with probability $(i(N - i) / N^2)$)
    \end{itemize}
    \item With these probabilities, we can write our rules of change as:
	    \[i_{t + 1} = \begin{cases}
            i_t - 1 & \frac{i (N - i)}{N^2} \\
            i_t + 1 & \frac{i (N - i)}{N^2} \\
            i_t & \frac{N^2 - 2iN + 2i^2}{N^2}
        \end{cases}\]
    \item If we draw a transition matrix, where every $i$ is a state, then every state has a loop and can only move to the two states next to it.  (Except for $i = 0$ or $i = N$, which are sinks.)
\end{itemize}
We can define a variable $X_i$ as the probability of ending up at $N$, when starting from $i$.
\begin{itemize}
	\item There are only two possible endpoints, $N$ and $0$.  $i$'s closer to zero will more likely end up at $0$ than $N$.
	\item Therefore, $X_0 = 0$, and $X_N = 1$.  What about at any other $i$?
	\item $X_i = P_{i, i - 1}X_{i - 1} + P_{i, i}X_{i} + P_{i, i + 1}X_{i + 1}$, where $P_{a, b}$ represents the probability of transitioning from state $a$ to $b$.
	\item Solving for $X_i$ is an exercise for the reader, but $X_i = \frac{i}{N}$.
	\item Notice that if $N$ is big, then the fraction is close to zero.  If $N$ is small, then the fraction is huge!  
    \begin{itemize}
        \item The implication of this is that population drift does not occur in large populations (large $N$), because usually $i$ starts so small that it is more likely to go to zero than to become dominant.
        \item Meanwhile, if the population is small, random mutations are much more powerful.
    \end{itemize}
\end{itemize}


\end{document}